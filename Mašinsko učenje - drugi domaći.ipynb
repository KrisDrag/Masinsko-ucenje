{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.metrics as metrics\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import math\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "**Reading and preprocessing data**\n",
    "\n",
    "\"ISBN\";\"Book-Title\";\"Book-Author\";\"Year-Of-Publication\";\"Publisher\";\"Image-URL-S\";\"Image-URL-M\";\"Image-URL-L\" - *Books.csv*\n",
    "\n",
    "\"User-ID\";\"Location\";\"Age\" - *Users.csv*\n",
    "\n",
    "\"User-ID\";\"ISBN\";\"Book-Rating\" - *Ratings.csv*\n",
    "\n",
    "#Loading data\n",
    "books = pd.read_csv('BX_Books.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "books.columns = ['ISBN', 'bookTitle', 'bookAuthor', 'yearOfPublication', 'publisher', 'imageUrlS', 'imageUrlM', 'imageUrlL']\n",
    "users = pd.read_csv('BX-Users.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "users.columns = ['userID', 'Location', 'Age']\n",
    "ratings = pd.read_csv('BX-Book-Ratings.csv', sep=';', error_bad_lines=False, encoding=\"latin-1\")\n",
    "ratings.columns = ['userID', 'ISBN', 'bookRating']\n",
    "\n",
    "#checking shapes of the datasets\n",
    "print (books.shape)\n",
    "print (users.shape)\n",
    "print (ratings.shape)\n",
    "\n",
    "#Exploring books dataset\n",
    "books.head()\n",
    "\n",
    "#dropping last three columns containing image URLs which will not be required for analysis\n",
    "books.drop(['imageUrlS', 'imageUrlM', 'imageUrlL'],axis=1,inplace=True)\n",
    "\n",
    "#Now the books datasets looks like....\n",
    "books.head()\n",
    "\n",
    "#checking data types of columns\n",
    "books.dtypes\n",
    "\n",
    "#making this setting to display full text in columns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "books.head()\n",
    "\n",
    "#yearOfPublication should be set as having dtype as int\n",
    "#checking the unique values of yearOfPublication\n",
    "books.yearOfPublication.unique()\n",
    "\n",
    "#Correcting the dtypes of yearOfPublication\n",
    "books.yearOfPublication=pd.to_numeric(books.yearOfPublication, errors='coerce')\n",
    "\n",
    "print (sorted(books['yearOfPublication'].unique()))\n",
    "\n",
    "#The value 0 is invalid and as this dataset was published in 2020, set the years after 2020 to be invalid\n",
    "#setting invalid years as NaN\n",
    "books.loc[(books.yearOfPublication > 2020) | (books.yearOfPublication == 0),'yearOfPublication'] = np.NAN\n",
    "\n",
    "#replacing NaNs with mean value of yearOfPublication\n",
    "books.yearOfPublication.fillna(round(books.yearOfPublication.mean()), inplace=True)\n",
    "\n",
    "#rechecking - no NANs\n",
    "books.yearOfPublication.isnull().sum()\n",
    "\n",
    "#resetting the dtype as int32\n",
    "books.yearOfPublication = books.yearOfPublication.astype(np.int32)\n",
    "\n",
    "#exploring 'publisher' column\n",
    "books.loc[books.publisher.isnull(),:]\n",
    "# two NANs\n",
    "\n",
    "#investigating rows having NaNs\n",
    "#Checking with rows having bookTitle as Tyrant Moon to see if we can get any concusion\n",
    "books.loc[(books.bookTitle == 'Tyrant Moon'),:]\n",
    "#no concusions\n",
    "\n",
    "#Checking rows having bookTitle as Finder Keepers to see if we can get any conclusion\n",
    "books.loc[(books.bookTitle == 'Finders Keepers'),:]\n",
    "#all rows with different publisher and bookAuthor\n",
    "\n",
    "#checking by bookAuthor to find patterns\n",
    "books.loc[(books.bookAuthor == 'Elaine Corvidae'),:]\n",
    "#all having different publisher...no conclusions here\n",
    "\n",
    "#checking by bookAuthor to find patterns\n",
    "books.loc[(books.bookAuthor == 'Linnea Sinclair'),:]\n",
    "\n",
    "#since there is nothing in common to infer publisher for NaNs, replacing these with 'other\n",
    "books.loc[(books.ISBN == '193169656X'),'publisher'] = 'other'\n",
    "books.loc[(books.ISBN == '1931696993'),'publisher'] = 'other'\n",
    "\n",
    "# label encoding - title, author, publisher\n",
    "label_encoder = LabelEncoder()\n",
    "books['bookTitleCoded'] = label_encoder.fit_transform(books[\"bookTitle\"].astype(np.str)).astype(np.int64)\n",
    "books['bookAuthorCoded'] = label_encoder.fit_transform(books[\"bookAuthor\"].astype(np.str)).astype(np.int64)\n",
    "books['publisherCoded'] = label_encoder.fit_transform(books[\"publisher\"].astype(np.str)).astype(np.int64)\n",
    "books.head()\n",
    "\n",
    "books.hist(bins = 50, figsize = (15,10))\n",
    "plt.show()\n",
    "\n",
    "# Finding parameter correlations\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "corr = books.corr()\n",
    "ax = sns.heatmap(corr, annot = True, cmap='Blues')\n",
    "\n",
    "print(users.shape)\n",
    "users.head()\n",
    "\n",
    "users.dtypes\n",
    "\n",
    "users.userID.values\n",
    "\n",
    "print (sorted(users.Age.unique()))\n",
    "\n",
    "#Age column has some invalid entries like nan, 0 and very high values like 100 and above\n",
    "#Values below 5 and above 90 do not make much sense for our book rating case...hence replacing these by NaNs\n",
    "users.loc[(users.Age > 90) | (users.Age < 5), 'Age'] = np.nan\n",
    "\n",
    "#replacing NaNs with mean\n",
    "users.Age = users.Age.fillna(users.Age.mean())\n",
    "\n",
    "#setting the data type as int\n",
    "users.Age = users.Age.astype(np.int32)\n",
    "\n",
    "#rechecking\n",
    "print(sorted(users.Age.unique()))\n",
    "\n",
    "users['Age'].hist(bins = 50, figsize = (15,10))\n",
    "plt.show()\n",
    "\n",
    "#checking shape\n",
    "ratings.shape\n",
    "\n",
    "#ratings dataset will have n_users*n_books entries if every user rated every item, this shows that the dataset is very sparse\n",
    "n_users = users.shape[0]\n",
    "n_books = books.shape[0]\n",
    "print(n_users * n_books)\n",
    "\n",
    "#checking first few rows...\n",
    "ratings.head(5)\n",
    "\n",
    "ratings.bookRating.unique()\n",
    "\n",
    "#ratings dataset should have books only which exist in our books dataset, unless new books are added to books dataset\n",
    "ratings_new = ratings[ratings.ISBN.isin(books.ISBN)]\n",
    "\n",
    "print (ratings.shape)\n",
    "print (ratings_new.shape)\n",
    "#it can be seen that many rows having book ISBN not part of books dataset got dropped off\n",
    "\n",
    "#ratings dataset should have ratings from users which exist in users dataset, unless new users are added to users dataset\n",
    "ratings = ratings[ratings.userID.isin(users.userID)]\n",
    "\n",
    "print(ratings.shape)\n",
    "print(ratings_new.shape)\n",
    "#no new users added, hence we will go with above dataset ratings_new (1031175, 3)\n",
    "\n",
    "print(\"number of users: \" + str(n_users))\n",
    "print(\"number of books: \" + str(n_books))\n",
    "\n",
    "#Sparsity of dataset in %\n",
    "sparsity=1.0-len(ratings_new)/float(n_users*n_books)\n",
    "print('The sparsity level of Book Crossing dataset is ' +  str(sparsity*100) + ' %')\n",
    "\n",
    "#BX-Book-Ratings contains the book rating information. Ratings are either explicit, expressed on a scale from 1-10 \n",
    "#higher values denoting higher appreciation, or implicit, expressed by 0\n",
    "ratings.bookRating.unique()\n",
    "\n",
    "#Hence segragating implicit and explict ratings datasets\n",
    "ratings_explicit = ratings_new[ratings_new.bookRating != 0]\n",
    "ratings_implicit = ratings_new[ratings_new.bookRating == 0]\n",
    "\n",
    "#checking shapes\n",
    "print(ratings_new.shape)\n",
    "print(ratings_explicit.shape)\n",
    "print(ratings_implicit.shape)\n",
    "\n",
    "#plotting count of bookRating\n",
    "sns.countplot(data=ratings_explicit , x='bookRating')\n",
    "plt.show()\n",
    "#It can be seen that higher ratings are more common amongst users and rating 8 has been rated highest number of times\n",
    "\n",
    "#At this point , a simple popularity based recommendation system can be built based on count of user ratings for different books\n",
    "ratings_count = pd.DataFrame(ratings_explicit.groupby(['ISBN'])['bookRating'].sum())\n",
    "top10 = ratings_count.sort_values('bookRating', ascending = False).head(10)\n",
    "print(\"Following books are recommended\")\n",
    "top10.merge(books, left_index = True, right_on = 'ISBN')\n",
    "\n",
    "#Similarly segregating users who have given explicit ratings from 1-10 and those whose implicit behavior was tracked\n",
    "users_exp_ratings = users[users.userID.isin(ratings_explicit.userID)]\n",
    "users_imp_ratings = users[users.userID.isin(ratings_implicit.userID)]\n",
    "\n",
    "#checking shapes\n",
    "print(users.shape)\n",
    "print(users_exp_ratings.shape)\n",
    "print(users_imp_ratings.shape)\n",
    "\n",
    "#To cope up with computing power I have and to reduce the dataset size, I am considering users who have rated atleast 100 books\n",
    "#and books which have atleast 100 ratings\n",
    "counts1 = ratings_explicit['userID'].value_counts()\n",
    "ratings_explicit = ratings_explicit[ratings_explicit['userID'].isin(counts1[counts1 >= 100].index)]\n",
    "counts = ratings_explicit['bookRating'].value_counts()\n",
    "ratings_explicit = ratings_explicit[ratings_explicit['bookRating'].isin(counts[counts >= 100].index)]\n",
    "\n",
    "#Merging tables from different files to include all the collumns we need \n",
    "allData = ratings_count.merge(books, left_index = True, right_on = 'ISBN')\n",
    "#dropping columns that won't be used\n",
    "allData.drop(['bookTitle', 'bookAuthor', 'publisher', 'ISBN'],axis=1,inplace=True)\n",
    "allData.head()\n",
    "\n",
    "#Merging tables from different files to include all the collumns we need \n",
    "df = ratings_count.merge(books, left_index = True, right_on = 'ISBN')\n",
    "df = df.merge(ratings_explicit, how='inner', left_on='ISBN', right_on='ISBN')\n",
    "df = df.merge(users_exp_ratings, how='inner', left_on='userID', right_on='userID')\n",
    "\n",
    "df['ISBNCoded'] = label_encoder.fit_transform(df[\"ISBN\"].astype(np.str)).astype(np.int64)\n",
    "df['bookRating'] = df['bookRating_x']\n",
    "df.drop(['bookTitle', 'bookAuthor', 'publisher', 'bookRating_x', 'bookRating_y', 'Location', 'ISBN', 'userID'],axis=1,inplace=True)\n",
    "\n",
    "df.head()\n",
    "\n",
    "df.dtypes\n",
    "\n",
    "# Finding parameter correlations\n",
    "fig, ax = plt.subplots(figsize=(14, 14))\n",
    "corr = df.corr()\n",
    "ax = sns.heatmap(corr, annot = True, cmap='Blues')\n",
    "\n",
    "# Scaling data\n",
    "min_max_scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(min_max_scaler.fit_transform(df), columns=df.columns, index=df.index)\n",
    "\n",
    "**K-Means**\n",
    "\n",
    "# finding optimal number of clusters\n",
    "distortions = []\n",
    "K = range(1,8)\n",
    "for k in K:\n",
    "    kmeanModel = KMeans(n_clusters=k)\n",
    "    kmeanModel.fit(df_scaled)\n",
    "    distortions.append(kmeanModel.inertia_)\n",
    "\n",
    "# Ploting Elbow Method in order to find optimal k\n",
    "plt.figure(figsize=(16,8))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Distortion')\n",
    "plt.title('The Elbow Method showing the optimal number of clusters')\n",
    "plt.show()\n",
    "\n",
    "kmeanModel = KMeans(n_clusters = 3, n_jobs=-1)\n",
    "kmeanModel.fit(df_scaled)\n",
    "\n",
    "clust_labels = kmeanModel.predict(df_scaled)\n",
    "centers = kmeanModel.cluster_centers_\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "scatter = ax.scatter(df_scaled['bookRating'], df_scaled['bookAuthorCoded'], c=pd.DataFrame(kmeanModel.predict(df_scaled)))\n",
    "ax.set_title('K-Means Clustering')\n",
    "ax.set_xlabel('Book Rating' )\n",
    "ax.set_ylabel('Age')\n",
    "ax.grid(True)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "scatter = ax.scatter(df_scaled['bookRating'], df_scaled['bookTitleCoded'], c=pd.DataFrame(kmeanModel.predict(df_scaled)))\n",
    "ax.set_title('K-Means Clustering')\n",
    "ax.set_xlabel('Book Rating')\n",
    "ax.set_ylabel('Title')\n",
    "ax.grid(True)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot()\n",
    "scatter = ax.scatter(df_scaled['bookRating'], df_scaled['bookAuthorCoded'], c=pd.DataFrame(kmeanModel.predict(df_scaled)))\n",
    "ax.set_title('K-Means Clustering')\n",
    "ax.set_xlabel('Book Rating')\n",
    "ax.set_ylabel('Author')\n",
    "ax.grid(True)\n",
    "\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = Axes3D(fig)\n",
    "ax.scatter(df_scaled['bookTitleCoded'], df_scaled['Age'], df_scaled['bookRating'], c=pd.DataFrame(kmeanModel.predict(df_scaled)))\n",
    "ax.set_title('K-Means Clustering')\n",
    "ax.set_xlabel('Book Author')\n",
    "ax.set_ylabel('Publisher')\n",
    "ax.set_zlabel('Book Rating')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
